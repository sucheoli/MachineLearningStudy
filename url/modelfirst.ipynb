{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981d022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install lightgbm\n",
    "# !pip install seaborn\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn\n",
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(train_data.info()) # 6,995,055개의 데이터\n",
    "print(test_data.info()) # 1,747,688개의 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ed709",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9389274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))  \n",
    "\n",
    "ax = sns.countplot(\n",
    "    x='label',\n",
    "    hue='label',\n",
    "    data=train_data,\n",
    "    palette='muted',\n",
    "    legend=False,  \n",
    ")\n",
    "\n",
    "plt.title('Label Distribution')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width() / 2,   # 막대 중앙 x 위치\n",
    "        p.get_height() + 0.2,            # 막대 위쪽 y 위치\n",
    "        f'{int(p.get_height())}',        # 개수 표시\n",
    "        ha='center', va='bottom'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8dab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['URL'] = train_data['URL'].str.replace('[.]', '.')\n",
    "test_data['URL'] = test_data['URL'].str.replace('[.]', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import math \n",
    "from collections import Counter\n",
    "\n",
    "# -----------------------------------\n",
    "# URL normalize (핵심!)\n",
    "# -----------------------------------\n",
    "\n",
    "def normalize_url(url):\n",
    "    if not re.match(r'^https?://', url):\n",
    "        return 'http://' + url\n",
    "    return url\n",
    "\n",
    "# -----------------------------------\n",
    "# Helper functions\n",
    "# -----------------------------------\n",
    "\n",
    "def count_digits(url):\n",
    "    return sum(char.isdigit() for char in url)\n",
    "\n",
    "def count_special_chars(url):\n",
    "    return sum(char in string.punctuation for char in url)\n",
    "\n",
    "def extract_domain(url):\n",
    "    match = re.search(r'://([^/]+)', url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def has_digit(s):\n",
    "    return any(c.isdigit() for c in str(s))\n",
    "\n",
    "def count_unique_chars(url):\n",
    "    return len(set(url))\n",
    "\n",
    "def extract_tld(url):\n",
    "    match = re.search(r'\\.([a-zA-Z]{2,10})(?:/|$)', url)\n",
    "    return match.group(1).lower() if match else None\n",
    "\n",
    "def path_part(url):\n",
    "    match = re.search(r'[^/]+/(.*)', url)\n",
    "    return match.group(1) if match else \"\"\n",
    "\n",
    "def query_part(url):\n",
    "    match = re.search(r'\\?(.*)', url)\n",
    "    return match.group(1) if match else \"\"\n",
    "\n",
    "def calculate_entropy(url):\n",
    "    # 문자 빈도수 계산\n",
    "    counter = Counter(url)\n",
    "    length = len(url)\n",
    "    \n",
    "    # 섀넌 엔트로피 계산\n",
    "    entropy = 0\n",
    "    for count in counter.values():\n",
    "        probability = count / length\n",
    "        entropy -= probability * math.log2(probability)\n",
    "    return entropy\n",
    "\n",
    "def check_randomness(url):\n",
    "    # 1. 엔트로피 계산\n",
    "    entropy = calculate_entropy(url)\n",
    "    \n",
    "    # 2. 연속된 숫자나 문자의 최대 길이\n",
    "    max_consecutive = 1\n",
    "    current_consecutive = 1\n",
    "    for i in range(1, len(url)):\n",
    "        if url[i].isalnum() and url[i-1].isalnum() and url[i].lower() == url[i-1].lower():\n",
    "            current_consecutive += 1\n",
    "            max_consecutive = max(max_consecutive, current_consecutive)\n",
    "        else:\n",
    "            current_consecutive = 1\n",
    "    \n",
    "    # 3. 숫자와 문자의 교차 패턴 수\n",
    "    transitions = 0\n",
    "    for i in range(1, len(url)):\n",
    "        if (url[i-1].isdigit() and url[i].isalpha()) or \\\n",
    "           (url[i-1].isalpha() and url[i].isdigit()):\n",
    "            transitions += 1\n",
    "    \n",
    "    # 무작위성 점수 계산 (0~1 사이로 정규화)\n",
    "    entropy_score = min(entropy / 5.0, 1.0)  # 일반적인 URL의 최대 엔트로피를 5로 가정\n",
    "    consecutive_score = min(max_consecutive / 10.0, 1.0)  # 연속된 문자가 많을수록 낮은 점수\n",
    "    transition_score = min(transitions / 10.0, 1.0)  # 전환이 많을수록 높은 점수\n",
    "    \n",
    "    # 종합 점수 계산 (가중치 조정 가능)\n",
    "    randomness_score = (entropy_score * 0.5 + \n",
    "                       (1 - consecutive_score) * 0.3 + \n",
    "                       transition_score * 0.2)\n",
    "    \n",
    "    return randomness_score\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# Feature Engineering\n",
    "# -----------------------------------\n",
    "\n",
    "def add_url_features(df):\n",
    "    df['URL_norm'] = df['URL'].apply(normalize_url)\n",
    "\n",
    "    # 길이 기반\n",
    "    df['length'] = df['URL'].str.len()\n",
    "    df['subdomain_count'] = df['URL'].str.split('.').str.len()\n",
    "    df['num_digits'] = df['URL'].apply(count_digits)\n",
    "    df['num_special_chars'] = df['URL'].apply(count_special_chars)\n",
    "    df['unique_chars'] = df['URL'].apply(count_unique_chars)\n",
    "\n",
    "    # domain\n",
    "    df['domain'] = df['URL_norm'].apply(extract_domain)\n",
    "    df['domain_length'] = df['domain'].astype(str).str.len()\n",
    "    df['domain_has_digit'] = df['domain'].apply(has_digit)\n",
    "    df['num_hyphens'] = df['URL'].str.count('-')\n",
    "    df['num_underscores'] = df['URL'].str.count('_')\n",
    "\n",
    "    # IP URL\n",
    "    df['is_ip_url'] = df['URL_norm'].str.contains(r'^\\d{1,3}(\\.\\d{1,3}){3}')\n",
    "\n",
    "    # http/https\n",
    "    df['is_http'] = df['URL_norm'].str.startswith('http://')\n",
    "    df['is_https'] = df['URL_norm'].str.startswith('https://')\n",
    "\n",
    "    # :port\n",
    "    df['has_port'] = df['URL_norm'].str.contains(r':\\d{2,5}')\n",
    "\n",
    "    # @\n",
    "    df['has_at'] = df['URL_norm'].str.contains('@')\n",
    "\n",
    "    # path & query\n",
    "    df['path'] = df['URL_norm'].apply(path_part)\n",
    "    df['path_length'] = df['path'].str.len()\n",
    "    df['query'] = df['URL_norm'].apply(query_part)\n",
    "    df['query_length'] = df['query'].str.len()\n",
    "    df['num_params'] = df['query'].str.count('=')\n",
    "\n",
    "    # tld\n",
    "    df['tld'] = df['URL_norm'].apply(extract_tld)\n",
    "\n",
    "    # 피싱 키워드\n",
    "    phishing_keywords = ['login', 'secure', 'update', 'verify', 'account', 'bank',\n",
    "                         'paypal', 'free', 'bonus', 'admin', 'redirect', 'auth',\n",
    "                         'signin','server','click','immediate','confirm']\n",
    "    df['has_phish_keyword'] = df['URL'].apply(\n",
    "        lambda x: any(k in x.lower() for k in phishing_keywords)\n",
    "    )\n",
    "    #+URL 무작위성\n",
    "    df['randomness'] = df['URL'].apply(check_randomness)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 실행\n",
    "# -----------------------------------\n",
    "\n",
    "train_data = add_url_features(train_data)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb56d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# =====================================================\n",
    "# 1) URL normalize (스킴 없는 경우 http:// 자동 추가)\n",
    "# =====================================================\n",
    "def normalize_url(url):\n",
    "    if not re.match(r'^https?://', url):\n",
    "        return 'http://' + url\n",
    "    return url\n",
    "\n",
    "# =====================================================\n",
    "# 2) Feature helper functions\n",
    "# =====================================================\n",
    "def count_digits(url):\n",
    "    return sum(char.isdigit() for char in url)\n",
    "\n",
    "def count_special_chars(url):\n",
    "    return sum(char in string.punctuation for char in url)\n",
    "\n",
    "def extract_domain(url):\n",
    "    m = re.search(r'://([^/]+)', url)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def extract_tld(url):\n",
    "    m = re.search(r'\\.([a-zA-Z]{2,10})(?:/|$)', url)\n",
    "    return m.group(1).lower() if m else None\n",
    "\n",
    "def count_unique_chars(url):\n",
    "    return len(set(url))\n",
    "\n",
    "def has_digit(s):\n",
    "    return any(c.isdigit() for c in str(s))\n",
    "\n",
    "def path_part(url):\n",
    "    m = re.search(r'[^/]+/(.*)', url)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def query_part(url):\n",
    "    m = re.search(r'\\?(.*)', url)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def calculate_entropy(url):\n",
    "    # 문자 빈도수 계산\n",
    "    counter = Counter(url)\n",
    "    length = len(url)\n",
    "    \n",
    "    # 섀넌 엔트로피 계산\n",
    "    entropy = 0\n",
    "    for count in counter.values():\n",
    "        probability = count / length\n",
    "        entropy -= probability * math.log2(probability)\n",
    "    return entropy\n",
    "\n",
    "def check_randomness(url):\n",
    "    # 1. 엔트로피 계산\n",
    "    entropy = calculate_entropy(url)\n",
    "    \n",
    "    # 2. 연속된 숫자나 문자의 최대 길이\n",
    "    max_consecutive = 1\n",
    "    current_consecutive = 1\n",
    "    for i in range(1, len(url)):\n",
    "        if url[i].isalnum() and url[i-1].isalnum() and url[i].lower() == url[i-1].lower():\n",
    "            current_consecutive += 1\n",
    "            max_consecutive = max(max_consecutive, current_consecutive)\n",
    "        else:\n",
    "            current_consecutive = 1\n",
    "    \n",
    "    # 3. 숫자와 문자의 교차 패턴 수\n",
    "    transitions = 0\n",
    "    for i in range(1, len(url)):\n",
    "        if (url[i-1].isdigit() and url[i].isalpha()) or \\\n",
    "           (url[i-1].isalpha() and url[i].isdigit()):\n",
    "            transitions += 1\n",
    "    \n",
    "    # 무작위성 점수 계산 (0~1 사이로 정규화)\n",
    "    entropy_score = min(entropy / 5.0, 1.0)  # 일반적인 URL의 최대 엔트로피를 5로 가정\n",
    "    consecutive_score = min(max_consecutive / 10.0, 1.0)  # 연속된 문자가 많을수록 낮은 점수\n",
    "    transition_score = min(transitions / 10.0, 1.0)  # 전환이 많을수록 높은 점수\n",
    "    \n",
    "    # 종합 점수 계산 (가중치 조정 가능)\n",
    "    randomness_score = (entropy_score * 0.5 + \n",
    "                       (1 - consecutive_score) * 0.3 + \n",
    "                       transition_score * 0.2)\n",
    "    \n",
    "    return randomness_score\n",
    "\n",
    "# =====================================================\n",
    "# 3) Feature Engineering\n",
    "# =====================================================\n",
    "def add_url_features(df):\n",
    "    df['URL_norm'] = df['URL'].apply(normalize_url)\n",
    "\n",
    "    df['length'] = df['URL'].str.len()\n",
    "    df['subdomain_count'] = df['URL'].str.split('.').str.len()\n",
    "    df['num_digits'] = df['URL'].apply(count_digits)\n",
    "    df['num_special_chars'] = df['URL'].apply(count_special_chars)\n",
    "    df['unique_chars'] = df['URL'].apply(count_unique_chars)\n",
    "\n",
    "    df['domain'] = df['URL_norm'].apply(extract_domain)\n",
    "    df['domain_length'] = df['domain'].astype(str).str.len()\n",
    "    df['domain_has_digit'] = df['domain'].apply(has_digit)\n",
    "\n",
    "    df['num_hyphens'] = df['URL'].str.count('-')\n",
    "    df['num_underscores'] = df['URL'].str.count('_')\n",
    "\n",
    "    df['is_ip_url'] = df['URL_norm'].str.contains(r'^\\d{1,3}(\\.\\d{1,3}){3}')\n",
    "    df['is_http'] = df['URL_norm'].str.startswith('http://')\n",
    "    df['is_https'] = df['URL_norm'].str.startswith('https://')\n",
    "    df['has_port'] = df['URL_norm'].str.contains(r':\\d{2,5}')\n",
    "    df['has_at'] = df['URL_norm'].str.contains('@')\n",
    "\n",
    "    df['path'] = df['URL_norm'].apply(path_part)\n",
    "    df['path_length'] = df['path'].str.len()\n",
    "    df['query'] = df['URL_norm'].apply(query_part)\n",
    "    df['query_length'] = df['query'].str.len()\n",
    "    df['num_params'] = df['query'].str.count('=')\n",
    "\n",
    "    df['tld'] = df['URL_norm'].apply(extract_tld)\n",
    "    \n",
    "    # ✅ phish keyword feature 추가 (train과 test 일치!)\n",
    "    phishing_keywords = ['login', 'secure', 'update', 'verify', 'account', 'bank',\n",
    "                         'paypal', 'free', 'bonus', 'admin', 'redirect', 'auth',\n",
    "                         'signin','server','click','immediate','confirm']\n",
    "    df['has_phish_keyword'] = df['URL'].apply(\n",
    "        lambda x: any(k in x.lower() for k in phishing_keywords)\n",
    "    )\n",
    "     #+URL 무작위성\n",
    "    df['randomness'] = df['URL'].apply(check_randomness)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 4) Feature Engineering 실행\n",
    "# =====================================================\n",
    "df = add_url_features(train_data.copy())\n",
    "\n",
    "# tld 인코딩 (문자열은 LightGBM이 못 먹음)\n",
    "le = LabelEncoder()\n",
    "df['tld_encoded'] = le.fit_transform(df['tld'].astype(str))\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 5) Feature 선택 (문자열 제거)\n",
    "# =====================================================\n",
    "exclude_cols = [\n",
    "    'ID',        # 필요 없음\n",
    "    'URL',\n",
    "    'URL_norm',\n",
    "    'domain',\n",
    "    'path',\n",
    "    'query',\n",
    "    'tld',\n",
    "    'label'\n",
    "]\n",
    "\n",
    "features = [c for c in df.columns if c not in exclude_cols]\n",
    "\n",
    "X = df[features]\n",
    "y = df['label']\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 6) Train / Valid split\n",
    "# =====================================================\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "valid_dataset = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 7) LightGBM Baseline 모델\n",
    "# =====================================================\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_dataset,\n",
    "    valid_sets=[valid_dataset],\n",
    "    valid_names=['valid'],\n",
    "    num_boost_round=500  \n",
    ")\n",
    "feature_list = features.copy()\n",
    "\n",
    "# =====================================================\n",
    "# 8) 평가\n",
    "# =====================================================\n",
    "valid_pred = model.predict(X_valid)\n",
    "valid_pred_label = (valid_pred > 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_valid, valid_pred_label)\n",
    "f1 = f1_score(y_valid, valid_pred_label)\n",
    "\n",
    "print(\"Validation Accuracy:\", acc)\n",
    "print(\"Validation F1:\", f1)\n",
    "\n",
    "# Feature Importance\n",
    "lgb.plot_importance(model, max_num_features=20, height=0.4, figsize=(10, 8))\n",
    "plt.title(\"LightGBM Feature Importance (Top 20)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d896e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_tree(model, tree_index=0, figsize=(30, 20), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c682848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1) test feature engineering\n",
    "# ------------------------------\n",
    "test_df = add_url_features(test_data.copy())\n",
    "\n",
    "# ------------------------------\n",
    "# 2) TLD 처리 (unseen → unknown)\n",
    "# ------------------------------\n",
    "test_df['tld_fixed'] = test_df['tld'].astype(str).apply(\n",
    "    lambda x: x if x in le.classes_ else \"unknown\"\n",
    ")\n",
    "\n",
    "# unknown 추가 후 re-fit\n",
    "new_classes = list(le.classes_)\n",
    "if \"unknown\" not in new_classes:\n",
    "    new_classes.append(\"unknown\")\n",
    "le.fit(new_classes)\n",
    "\n",
    "test_df['tld_encoded'] = le.transform(test_df['tld_fixed'])\n",
    "\n",
    "# ------------------------------\n",
    "# 3) feature 리스트는 train에서 그대로 불러옴\n",
    "# ------------------------------\n",
    "\n",
    "X_test = test_df[feature_list]\n",
    "\n",
    "# ------------------------------\n",
    "# 4) predict\n",
    "# ------------------------------\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "# ------------------------------\n",
    "# 5) submission 생성\n",
    "# ------------------------------\n",
    "submission = pd.DataFrame({               \n",
    "    'ID': test_df['ID'],\n",
    "    'probability': test_pred\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (venv)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
